{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display:block\" direction=rtl align=right><br><br>\n",
        "    <div  style=\"width:100%;margin:100;display:block\"  display=block align=center>\n",
        "        <img width=130 align=right src=\"https://i.ibb.co/yXKQmtZ/logo1.png\" style=\"margin:0;\" />\n",
        "        <img width=170 align=left  src=\"https://i.ibb.co/wLjqFkw/logo2.png\" style=\"margin:0;\" />\n",
        "        <span><br><font size=5>University of Tehran , school of ECE</font></span>\n",
        "        <span><br><font size=3>HekiDesk Project</font></span>\n",
        "        <span><br><font size=3>ECG Processing and Classification</font></span>\n",
        "        <span><br><font size=3>Spring 2023</font></span>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "CipvQLjBTpi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "ErAZ9HmCT9N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "import multiprocessing as mp\n",
        "def shuffle_data(data, labels):\n",
        "    \"\"\"\n",
        "    Shuffles input data\n",
        "    In some cases input data might be distributed sorted which might create a hidden error\n",
        "    in training/validation process so it's better to always shuffle input data before usage\n",
        "    :return: Shuffled input data\n",
        "    \"\"\"\n",
        "    data_shuf = []\n",
        "    labels_shuf = []\n",
        "    index_shuf = list(range(len(data)))\n",
        "    random.shuffle(index_shuf)\n",
        "    for i in index_shuf:\n",
        "        data_shuf.append(data[i])\n",
        "        labels_shuf.append(labels[i])\n",
        "    return (np.array(data_shuf), labels_shuf)\n",
        "\n",
        "__MAPPING__ = {\n",
        "    'A': 0,\n",
        "    'N': 1,\n",
        "    'O': 2,\n",
        "    '~': 3\n",
        "}\n",
        "\n",
        "__REVERSE_MAPPING__ = {\n",
        "    0: 'A',\n",
        "    1: 'N',\n",
        "    2: 'O',\n",
        "    3: '~'\n",
        "}\n",
        "\n",
        "def format_labels(labels):\n",
        "    return [__MAPPING__[x] for x in labels]\n",
        "\n",
        "def get_original_label(category):\n",
        "    return __REVERSE_MAPPING__[category]\n",
        "\n",
        "def show_balancing(y):\n",
        "    counter = Counter(y)\n",
        "    for key in sorted(list(counter.keys())):\n",
        "        print(key, counter[key])\n",
        "\n",
        "def find(a, condition):\n",
        "    \"\"\"\n",
        "    Analog to matlab: find(array condition)\n",
        "    eg find(array == 0) -> find(array, lambda x: x == 0)\n",
        "    :return: array of positions where condition is true\n",
        "    \"\"\"\n",
        "    return [i for (i, val) in enumerate(a) if condition(val)]\n",
        "\n",
        "def balance2(x, y):\n",
        "    uniq = np.unique(y)\n",
        "    selected = dict()\n",
        "    for val in uniq:\n",
        "        selected[val] = [x[i] for i in find(y, lambda v: v == val)]\n",
        "    min_len = 1000 * min([len(x) for x in selected.values()])\n",
        "    x = []\n",
        "    y = []\n",
        "    for (key, value) in selected.items():\n",
        "        slen = min(len(value), min_len)\n",
        "        x += value[:slen]\n",
        "        y += [key for i in range(slen)]\n",
        "    x, y = shuffle_data(x, y)\n",
        "    return x, y\n",
        "\n",
        "def get_number_of_jobs():\n",
        "    return int(mp.cpu_count())\n",
        "\n",
        "def apply_async(array, func):\n",
        "    pool = mp.Pool(get_number_of_jobs())\n",
        "    result = pool.map(func, array)\n",
        "    pool.close()\n",
        "    return result\n",
        "\n",
        "def __remove_dc_component(ecg):\n",
        "    mean = np.mean(ecg)\n",
        "    # cancel DC components\n",
        "    return ecg - mean\n",
        "def max_normalization(ecg):\n",
        "    return ecg / max(np.fabs(np.amin(ecg)), np.fabs(np.amax(ecg)))\n",
        "def normalize_ecg(ecg):\n",
        "    \"\"\"\n",
        "    Normalizes to a range of [-1; 1]\n",
        "    :param ecg: input signal\n",
        "    :return: normalized signal\n",
        "    \"\"\"\n",
        "    ecg = __remove_dc_component(ecg)\n",
        "    ecg = max_normalization(ecg)\n",
        "    return ecg\n",
        "\n",
        "def normalize_batch(array):\n",
        "    return apply_async(array, normalize_ecg)\n",
        "\n"
      ],
      "metadata": {
        "id": "nbwFLOhsvJ5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from six.moves import map, range, zip\n",
        "import six\n",
        "\n",
        "# built-in\n",
        "import collections\n",
        "import copy\n",
        "import keyword\n",
        "import os\n",
        "\n",
        "class ReturnTuple(tuple):\n",
        "\n",
        "    def __new__(cls, values, names=None):\n",
        "\n",
        "        return tuple.__new__(cls, tuple(values))\n",
        "\n",
        "    def __init__(self, values, names=None):\n",
        "\n",
        "        nargs = len(values)\n",
        "\n",
        "        if names is None:\n",
        "            # create names\n",
        "            names = ['_%d' % i for i in range(nargs)]\n",
        "        else:\n",
        "            # check length\n",
        "            if len(names) != nargs:\n",
        "                raise ValueError(\"Number of names and values mismatch.\")\n",
        "\n",
        "            # convert to str\n",
        "            names = list(map(str, names))\n",
        "\n",
        "            # check for keywords, alphanumeric, digits, repeats\n",
        "            seen = set()\n",
        "            for name in names:\n",
        "                if not all(c.isalnum() or (c == '_') for c in name):\n",
        "                    raise ValueError(\"Names can only contain alphanumeric \\\n",
        "                                      characters and underscores: %r.\" % name)\n",
        "\n",
        "                if keyword.iskeyword(name):\n",
        "                    raise ValueError(\"Names cannot be a keyword: %r.\" % name)\n",
        "\n",
        "                if name[0].isdigit():\n",
        "                    raise ValueError(\"Names cannot start with a number: %r.\" %\n",
        "                                     name)\n",
        "\n",
        "                if name in seen:\n",
        "                    raise ValueError(\"Encountered duplicate name: %r.\" % name)\n",
        "\n",
        "                seen.add(name)\n",
        "\n",
        "        self._names = names\n",
        "\n",
        "    def as_dict(self):\n",
        "        \"\"\"Convert to an ordered dictionary.\n",
        "        Returns\n",
        "        -------\n",
        "        out : OrderedDict\n",
        "            An OrderedDict representing the return values.\n",
        "        \"\"\"\n",
        "\n",
        "        return collections.OrderedDict(zip(self._names, self))\n",
        "\n",
        "    __dict__ = property(as_dict)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, six.string_types):\n",
        "            if key not in self._names:\n",
        "                raise KeyError(\"Unknown key: %r.\" % key)\n",
        "\n",
        "            key = self._names.index(key)\n",
        "\n",
        "        return super(ReturnTuple, self).__getitem__(key)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return representation string.\"\"\"\n",
        "\n",
        "        tpl = '%s=%r'\n",
        "\n",
        "        rp = ', '.join(tpl % item for item in zip(self._names, self))\n",
        "\n",
        "        return 'ReturnTuple(%s)' % rp\n",
        "\n",
        "    def __getnewargs__(self):\n",
        "        \"\"\"Return self as a plain tuple; used for copy and pickle.\"\"\"\n",
        "\n",
        "        return tuple(self)\n",
        "\n",
        "    def keys(self):\n",
        "        \"\"\"Return the value names.\n",
        "        Returns\n",
        "        -------\n",
        "        out : list\n",
        "            The keys in the mapping.\n",
        "        \"\"\"\n",
        "\n",
        "        return list(self._names)"
      ],
      "metadata": {
        "id": "noRdNy9sxMSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feture Extraction"
      ],
      "metadata": {
        "id": "_P4ZXWTLUMv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.fftpack import rfft\n",
        "from scipy import stats\n",
        "from collections.abc import Iterable\n",
        "import scipy.signal as ss\n",
        "\n",
        "\n",
        "FREQUENCY = 300\n",
        "def frequency_powers(x, n_power_features=40):\n",
        "    fxx, pxx = signal.welch(x, FREQUENCY)\n",
        "    features = dict()\n",
        "    for i, v in enumerate(pxx[:n_power_features]):\n",
        "        features['welch' + str(i)] = v\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def frequency_powers_summary(x):\n",
        "    ecg_fs_range = (0, 50)\n",
        "    band_size = 5\n",
        "\n",
        "    features = dict()\n",
        "\n",
        "    fxx, pxx = signal.welch(x, FREQUENCY)\n",
        "    for i in range((ecg_fs_range[1] - ecg_fs_range[0]) // 5):\n",
        "        fs_min = i * band_size\n",
        "        fs_max = fs_min + band_size\n",
        "        indices = np.logical_and(fxx >= fs_min, fxx < fs_max)\n",
        "        bp = np.sum(pxx[indices])\n",
        "        features[\"power_\" + str(fs_min) + \"_\" + str(fs_max)] = bp\n",
        "\n",
        "    return features\n",
        "\n",
        "##############################################################################\n",
        "def extract_fft(x):\n",
        "    return rfft(x)[:len(x) // 2]\n",
        "def fft_features(beat):\n",
        "    pff = extract_fft(beat[:int(0.13 * FREQUENCY)])\n",
        "    rff = extract_fft(beat[int(0.13 * FREQUENCY):int(0.27 * FREQUENCY)])\n",
        "    tff = extract_fft(beat[int(0.27 * FREQUENCY):])\n",
        "\n",
        "    features = dict()\n",
        "    for i, v in enumerate(pff[:10]):\n",
        "        features['pft' + str(i)] = v\n",
        "\n",
        "    for i, v in enumerate(rff[:10]):\n",
        "        features['rft' + str(i)] = v\n",
        "\n",
        "    for i, v in enumerate(tff[:20]):\n",
        "        features['tft' + str(i)] = v\n",
        "\n",
        "    return features\n",
        "\n",
        "##############################################################################\n",
        "def mode(a):\n",
        "    return stats.mode(a, axis=None)[0][0]\n",
        "def heart_rate_features(hr):\n",
        "    features = {\n",
        "        'hr_max': 0,\n",
        "        'hr_min': 0,\n",
        "        'hr_mean': 0,\n",
        "        'hr_median': 0,\n",
        "        'hr_mode': 0,\n",
        "        'hr_std': 0\n",
        "    }\n",
        "\n",
        "    if len(hr) > 0:\n",
        "        features['hr_max'] = np.amax(hr)\n",
        "        features['hr_min'] = np.amin(hr)\n",
        "        features['hr_mean'] = np.mean(hr)\n",
        "        features['hr_median'] = np.median(hr)\n",
        "        features['hr_mode'] = mode(hr)\n",
        "        features['hr_std'] = np.std(hr)\n",
        "\n",
        "    return features\n",
        "##############################################################################\n",
        "def median_heartbeat(thb):\n",
        "    if len(thb) == 0:\n",
        "        return np.zeros((int(0.6 * FREQUENCY)), dtype=np.int32)\n",
        "\n",
        "    m = [np.median(col) for col in thb.T]\n",
        "\n",
        "    dists = [np.sum(np.square(s - m)) for s in thb]\n",
        "    pmin = np.argmin(dists)\n",
        "\n",
        "    median = thb[pmin]\n",
        "\n",
        "    r_pos = int(0.2 * FREQUENCY)\n",
        "    if median[r_pos] < 0:\n",
        "        median *= -1\n",
        "\n",
        "    return median\n",
        "def heart_beats_features(thb):\n",
        "    means = median_heartbeat(thb)\n",
        "    mins = np.array([col.min() for col in thb.T])\n",
        "    maxs = np.array([col.max() for col in thb.T])\n",
        "    # stds = np.array([col.std() for col in thb.T])\n",
        "    diff = maxs - mins\n",
        "\n",
        "    features = dict()\n",
        "    for i, v in enumerate(means):\n",
        "        features['median' + str(i)] = v\n",
        "\n",
        "    for i, v in enumerate(diff):\n",
        "        features['hbdiff' + str(i)] = v\n",
        "\n",
        "    return features\n",
        "\n",
        "##############################################################################\n",
        "def calcActivity(epoch):\n",
        "    \"\"\"\n",
        "    Calculate Hjorth activity over epoch\n",
        "    \"\"\"\n",
        "    return np.nanvar(epoch, axis=0)\n",
        "def calcMobility(epoch):\n",
        "    \"\"\"\n",
        "    Calculate the Hjorth mobility parameter over epoch\n",
        "    \"\"\"\n",
        "    # Mobility\n",
        "    # N.B. the sqrt of the variance is the standard deviation. So let's just get std(dy/dt) / std(y)\n",
        "    return np.divide(\n",
        "        np.nanstd(np.diff(epoch, axis=0)),\n",
        "        np.nanstd(epoch, axis=0))\n",
        "def calcComplexity(epoch):\n",
        "    \"\"\"\n",
        "    Calculate Hjorth complexity over epoch\n",
        "    \"\"\"\n",
        "    return np.divide(\n",
        "        calcMobility(np.diff(epoch, axis=0)),\n",
        "        calcMobility(epoch))\n",
        "def heart_beats_features2(thb):\n",
        "    means = median_heartbeat(thb)\n",
        "    stds = np.array([np.std(col) for col in thb.T])\n",
        "\n",
        "    r_pos = int(0.2 * FREQUENCY)\n",
        "\n",
        "    PQ = means[:int(0.15 * FREQUENCY)]\n",
        "    ST = means[int(0.25 * FREQUENCY):]\n",
        "\n",
        "    QR = means[int(0.13 * FREQUENCY):r_pos]\n",
        "    RS = means[r_pos:int(0.27 * FREQUENCY)]\n",
        "\n",
        "    q_pos = int(0.13 * FREQUENCY) + np.argmin(QR)\n",
        "    s_pos = r_pos + np.argmin(RS)\n",
        "\n",
        "    p_pos = np.argmax(PQ)\n",
        "    t_pos = np.argmax(ST)\n",
        "\n",
        "    t_wave = ST[max(0, t_pos - int(0.08 * FREQUENCY)):min(len(ST), t_pos + int(0.08 * FREQUENCY))]\n",
        "    p_wave = PQ[max(0, p_pos - int(0.06 * FREQUENCY)):min(len(PQ), p_pos + int(0.06 * FREQUENCY))]\n",
        "\n",
        "    r_plus = sum(1 if b[r_pos] > 0 else 0 for b in thb)\n",
        "    r_minus = len(thb) - r_plus\n",
        "\n",
        "    QRS = means[q_pos:s_pos]\n",
        "\n",
        "    a = dict()\n",
        "    a['PR_interval'] = r_pos - p_pos\n",
        "    a['P_max'] = PQ[p_pos]\n",
        "    a['P_to_R'] = PQ[p_pos] / means[r_pos]\n",
        "    a['P_to_Q'] = PQ[p_pos] - means[q_pos]\n",
        "    a['ST_interval'] = t_pos\n",
        "    a['T_max'] = ST[t_pos]\n",
        "    a['R_plus'] = r_plus / max(1, len(thb))\n",
        "    a['R_minus'] = r_minus / max(1, len(thb))\n",
        "    a['T_to_R'] = ST[t_pos] / means[r_pos]\n",
        "    a['T_to_S'] = ST[t_pos] - means[s_pos]\n",
        "    a['P_to_T'] = PQ[p_pos] / ST[t_pos]\n",
        "    a['P_skew'] = skew(p_wave)\n",
        "    a['P_kurt'] = kurtosis(p_wave)\n",
        "    a['T_skew'] = skew(t_wave)\n",
        "    a['T_kurt'] = kurtosis(t_wave)\n",
        "    a['activity'] = calcActivity(means)\n",
        "    a['mobility'] = calcMobility(means)\n",
        "    a['complexity'] = calcComplexity(means)\n",
        "    a['QRS_len'] = s_pos - q_pos\n",
        "\n",
        "    qrs_min = abs(min(QRS))\n",
        "    qrs_max = abs(max(QRS))\n",
        "    qrs_abs = max(qrs_min, qrs_max)\n",
        "    sign = -1 if qrs_min > qrs_max else 1\n",
        "\n",
        "    a['QRS_diff'] = sign * abs(qrs_min / qrs_abs)\n",
        "    a['QS_diff'] = abs(means[s_pos] - means[q_pos])\n",
        "    a['QRS_kurt'] = kurtosis(QRS)\n",
        "    a['QRS_skew'] = skew(QRS)\n",
        "    a['QRS_minmax'] = qrs_max - qrs_min\n",
        "    a['P_std'] = np.mean(stds[:q_pos])\n",
        "    a['T_std'] = np.mean(stds[s_pos:])\n",
        "\n",
        "    return a\n",
        "\n",
        "##############################################################################\n",
        "def heart_beats_features3(thb):\n",
        "    means = np.array([np.mean(col) for col in thb.T])\n",
        "    medians = np.array([np.median(col) for col in thb.T])\n",
        "\n",
        "    diff = np.subtract(means, medians)\n",
        "    diff = np.power(diff, 2)\n",
        "\n",
        "    return {\n",
        "        'mean_median_diff_mean': np.mean(diff),\n",
        "        'mean_median_diff_std': np.std(diff)\n",
        "    }\n",
        "##############################################################################\n",
        "\n",
        "def cross_beats(s, peaks):\n",
        "    fs = FREQUENCY\n",
        "    r_after = int(0.06 * fs)\n",
        "    r_before = int(0.06 * fs)\n",
        "\n",
        "    crossbeats = []\n",
        "    for i in range(1, len(peaks)):\n",
        "        start = peaks[i - 1] + r_after\n",
        "        end = peaks[i] - r_before\n",
        "        if start >= end:\n",
        "            continue\n",
        "\n",
        "        crossbeats.append(s[start:end])\n",
        "\n",
        "    features = dict()\n",
        "    f_peaks = [sign_changes(x) for x in crossbeats]\n",
        "    features['cb_p_mean'] = np.mean(f_peaks)\n",
        "    features['cb_p_min'] = np.min(f_peaks)\n",
        "    features['cb_p_max'] = np.max(f_peaks)\n",
        "\n",
        "    return features\n",
        "\n",
        "##############################################################################\n",
        "def sign_changes(x):\n",
        "    return len(list(itertools.groupby(x, lambda x: x > 0))) - (x[0] > 0)\n",
        "\n",
        "##############################################################################\n",
        "def time_domain(rri: Iterable):\n",
        "\n",
        "    rmssd = 0\n",
        "    sdnn = 0\n",
        "    nn20 = 0\n",
        "    pnn20 = 0\n",
        "    nn50 = 0\n",
        "    pnn50 = 0\n",
        "    mrri = 0\n",
        "    stdrri = 0\n",
        "    mhr = 0\n",
        "\n",
        "    if len(rri) > 0:\n",
        "        diff_rri = np.diff(rri)\n",
        "        if len(diff_rri) > 0:\n",
        "            # Root mean square of successive differences\n",
        "            rmssd = np.sqrt(np.mean(diff_rri ** 2))\n",
        "            # Number of pairs of successive NNs that differ by more than 50ms\n",
        "            nn50 = sum(abs(diff_rri) > 50)\n",
        "            # Proportion of NN50 divided by total number of NNs\n",
        "            pnn50 = (nn50 / len(diff_rri)) * 100\n",
        "\n",
        "            # Number of pairs of successive NNs that differe by more than 20ms\n",
        "            nn20 = sum(abs(diff_rri) > 20)\n",
        "            # Proportion of NN20 divided by total number of NNs\n",
        "            pnn20 = (nn20 / len(diff_rri)) * 100\n",
        "\n",
        "        # Standard deviation of NN intervals\n",
        "        sdnn = np.std(rri, ddof=1)  # make it calculates N-1\n",
        "        # Mean of RR intervals\n",
        "        mrri = np.mean(rri)\n",
        "        # Std of RR intervals\n",
        "        stdrri = np.std(rri)\n",
        "        # Mean heart rate, in ms\n",
        "        mhr = 60 * 1000.0 / mrri\n",
        "\n",
        "    keys = ['rmssd', 'sdnn', 'nn20', 'pnn20', 'nn50', 'pnn50', 'mrri', 'stdrri', 'mhr']\n",
        "    values = [rmssd, sdnn, nn20, pnn20, nn50, pnn50, mrri, stdrri, mhr]\n",
        "    values = np.round(values, 2)\n",
        "    values = np.nan_to_num(values)\n",
        "\n",
        "    return dict(zip(keys, values))\n",
        "def r_features(s, r_peaks):\n",
        "    r_vals = [s[i] for i in r_peaks]\n",
        "\n",
        "    times = np.diff(r_peaks)\n",
        "    avg = np.mean(times)\n",
        "    filtered = sum([1 if i < 0.5 * avg else 0 for i in times])\n",
        "\n",
        "    total = len(r_vals) if len(r_vals) > 0 else 1\n",
        "\n",
        "    data = time_domain(times)\n",
        "\n",
        "    data['beats_to_length'] = len(r_peaks) / len(s)\n",
        "    data['r_mean'] = np.mean(r_vals)\n",
        "    data['r_std'] = np.std(r_vals)\n",
        "    data['filtered_r'] = filtered\n",
        "    data['rel_filtered_r'] = filtered / total\n",
        "\n",
        "    return data\n",
        "\n",
        "##############################################################################\n",
        "def add_suffix(dic, suffix):\n",
        "    keys = list(dic.keys())\n",
        "    for key in keys:\n",
        "        dic[key + suffix] = dic.pop(key)\n",
        "    return dic\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# def plot_ecg(ts=None,\n",
        "#              raw=None,\n",
        "#              filtered=None,\n",
        "#              rpeaks=None,\n",
        "#              templates_ts=None,\n",
        "#              templates=None,\n",
        "#              heart_rate_ts=None,\n",
        "#              heart_rate=None,\n",
        "#              path=None,\n",
        "#              show=False):\n",
        "#     \"\"\"Create a summary plot from the output of signals.ecg.ecg.\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     ts : array\n",
        "#         Signal time axis reference (seconds).\n",
        "#     raw : array\n",
        "#         Raw ECG signal.\n",
        "#     filtered : array\n",
        "#         Filtered ECG signal.\n",
        "#     rpeaks : array\n",
        "#         R-peak location indices.\n",
        "#     templates_ts : array\n",
        "#         Templates time axis reference (seconds).\n",
        "#     templates : array\n",
        "#         Extracted heartbeat templates.\n",
        "#     heart_rate_ts : array\n",
        "#         Heart rate time axis reference (seconds).\n",
        "#     heart_rate : array\n",
        "#         Instantaneous heart rate (bpm).\n",
        "#     path : str, optional\n",
        "#         If provided, the plot will be saved to the specified file.\n",
        "#     show : bool, optional\n",
        "#         If True, show the plot immediately.\n",
        "#     \"\"\"\n",
        "\n",
        "#     fig = plt.figure()\n",
        "#     fig.suptitle('ECG Summary')\n",
        "#     gs = gridspec.GridSpec(6, 2)\n",
        "\n",
        "#     # raw signal\n",
        "#     ax1 = fig.add_subplot(gs[:2, 0])\n",
        "\n",
        "#     ax1.plot(ts, raw, linewidth=MAJOR_LW, label='Raw')\n",
        "\n",
        "#     ax1.set_ylabel('Amplitude')\n",
        "#     ax1.legend()\n",
        "#     ax1.grid()\n",
        "\n",
        "#     # filtered signal with rpeaks\n",
        "#     ax2 = fig.add_subplot(gs[2:4, 0], sharex=ax1)\n",
        "\n",
        "#     ymin = np.min(filtered)\n",
        "#     ymax = np.max(filtered)\n",
        "#     alpha = 0.1 * (ymax - ymin)\n",
        "#     ymax += alpha\n",
        "#     ymin -= alpha\n",
        "\n",
        "#     ax2.plot(ts, filtered, linewidth=MAJOR_LW, label='Filtered')\n",
        "#     ax2.vlines(ts[rpeaks], ymin, ymax,\n",
        "#                color='m',\n",
        "#                linewidth=MINOR_LW,\n",
        "#                label='R-peaks')\n",
        "\n",
        "#     ax2.set_ylabel('Amplitude')\n",
        "#     ax2.legend()\n",
        "#     ax2.grid()\n",
        "\n",
        "#     # heart rate\n",
        "#     ax3 = fig.add_subplot(gs[4:, 0], sharex=ax1)\n",
        "\n",
        "#     ax3.plot(heart_rate_ts, heart_rate, linewidth=MAJOR_LW, label='Heart Rate')\n",
        "\n",
        "#     ax3.set_xlabel('Time (s)')\n",
        "#     ax3.set_ylabel('Heart Rate (bmp)')\n",
        "#     ax3.legend()\n",
        "#     ax3.grid()\n",
        "\n",
        "#     # templates\n",
        "#     ax4 = fig.add_subplot(gs[1:5, 1])\n",
        "\n",
        "#     ax4.plot(templates_ts, templates.T, 'm', linewidth=MINOR_LW, alpha=0.7)\n",
        "\n",
        "#     ax4.set_xlabel('Time (s)')\n",
        "#     ax4.set_ylabel('Amplitude')\n",
        "#     ax4.set_title('Templates')\n",
        "#     ax4.grid()\n",
        "\n",
        "#     # make layout tight\n",
        "#     gs.tight_layout(fig)\n",
        "\n",
        "#     # save to file\n",
        "#     if path is not None:\n",
        "#         path = utils.normpath(path)\n",
        "#         root, ext = os.path.splitext(path)\n",
        "#         ext = ext.lower()\n",
        "#         if ext not in ['png', 'jpg']:\n",
        "#             path = root + '.png'\n",
        "\n",
        "#         fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "#     # show\n",
        "#     if show:\n",
        "#         plt.show()\n",
        "#     else:\n",
        "#         # close\n",
        "#         plt.close(fig)\n",
        "# def plot(ts, signal, filtered, rpeaks, ts_tmpl, templates, ts_hr, hr):\n",
        "#     from .. import plotting\n",
        "#     plotting.plot_ecg(ts=ts,\n",
        "#                       raw=signal,\n",
        "#                       filtered=filtered,\n",
        "#                       rpeaks=rpeaks,\n",
        "#                       templates_ts=ts_tmpl,\n",
        "#                       templates=templates,\n",
        "#                       heart_rate_ts=ts_hr,\n",
        "#                       heart_rate=hr,\n",
        "#                       path=None,\n",
        "#                       show=True)\n",
        "\n",
        "def _norm_freq(frequency=None, sampling_rate=1000.):\n",
        "    # check inputs\n",
        "    if frequency is None:\n",
        "        raise TypeError(\"Please specify a frequency to normalize.\")\n",
        "\n",
        "    # convert inputs to correct representation\n",
        "    try:\n",
        "        frequency = float(frequency)\n",
        "    except TypeError:\n",
        "        # maybe frequency is a list or array\n",
        "        frequency = np.array(frequency, dtype='float')\n",
        "\n",
        "    Fs = float(sampling_rate)\n",
        "\n",
        "    wn = 2. * frequency / Fs\n",
        "\n",
        "    return wn\n",
        "def get_filter(ftype='FIR',\n",
        "               band='lowpass',\n",
        "               order=None,\n",
        "               frequency=None,\n",
        "               sampling_rate=1000., **kwargs):\n",
        "    # check inputs\n",
        "    if order is None:\n",
        "        raise TypeError(\"Please specify the filter order.\")\n",
        "    if frequency is None:\n",
        "        raise TypeError(\"Please specify the cutoff frequency.\")\n",
        "    if band not in ['lowpass', 'highpass', 'bandpass', 'bandstop']:\n",
        "        raise ValueError(\n",
        "            \"Unknown filter type '%r'; choose 'lowpass', 'highpass', \\\n",
        "            'bandpass', or 'bandstop'.\"\n",
        "            % band)\n",
        "\n",
        "    # convert frequencies\n",
        "    frequency = _norm_freq(frequency, sampling_rate)\n",
        "\n",
        "    # get coeffs\n",
        "    b, a = [], []\n",
        "    if ftype == 'FIR':\n",
        "        # FIR filter\n",
        "        if order % 2 == 0:\n",
        "            order += 1\n",
        "        a = np.array([1])\n",
        "        if band in ['lowpass', 'bandstop']:\n",
        "            b = ss.firwin(numtaps=order,\n",
        "                          cutoff=frequency,\n",
        "                          pass_zero=True, **kwargs)\n",
        "        elif band in ['highpass', 'bandpass']:\n",
        "            b = ss.firwin(numtaps=order,\n",
        "                          cutoff=frequency,\n",
        "                          pass_zero=False, **kwargs)\n",
        "    elif ftype == 'butter':\n",
        "        # Butterworth filter\n",
        "        b, a = ss.butter(N=order,\n",
        "                         Wn=frequency,\n",
        "                         btype=band,\n",
        "                         analog=False,\n",
        "                         output='ba', **kwargs)\n",
        "    elif ftype == 'cheby1':\n",
        "        # Chebyshev type I filter\n",
        "        b, a = ss.cheby1(N=order,\n",
        "                         Wn=frequency,\n",
        "                         btype=band,\n",
        "                         analog=False,\n",
        "                         output='ba', **kwargs)\n",
        "    elif ftype == 'cheby2':\n",
        "        # chevyshev type II filter\n",
        "        b, a = ss.cheby2(N=order,\n",
        "                         Wn=frequency,\n",
        "                         btype=band,\n",
        "                         analog=False,\n",
        "                         output='ba', **kwargs)\n",
        "    elif ftype == 'ellip':\n",
        "        # Elliptic filter\n",
        "        b, a = ss.ellip(N=order,\n",
        "                        Wn=frequency,\n",
        "                        btype=band,\n",
        "                        analog=False,\n",
        "                        output='ba', **kwargs)\n",
        "    elif ftype == 'bessel':\n",
        "        # Bessel filter\n",
        "        b, a = ss.bessel(N=order,\n",
        "                         Wn=frequency,\n",
        "                         btype=band,\n",
        "                         analog=False,\n",
        "                         output='ba', **kwargs)\n",
        "\n",
        "    return ReturnTuple((b, a), ('b', 'a'))\n",
        "def _filter_signal(b, a, signal, zi=None, check_phase=True, **kwargs):\n",
        "    # check inputs\n",
        "    if check_phase and zi is not None:\n",
        "        raise ValueError(\n",
        "            \"Incompatible arguments: initial filter state cannot be set when \\\n",
        "            check_phase is True.\")\n",
        "\n",
        "    if zi is None:\n",
        "        zf = None\n",
        "        if check_phase:\n",
        "            filtered = ss.filtfilt(b, a, signal, **kwargs)\n",
        "        else:\n",
        "            filtered = ss.lfilter(b, a, signal, **kwargs)\n",
        "    else:\n",
        "        filtered, zf = ss.lfilter(b, a, signal, zi=zi, **kwargs)\n",
        "\n",
        "    return filtered, zf\n",
        "def filter_signal(signal=None,\n",
        "                  ftype='FIR',\n",
        "                  band='lowpass',\n",
        "                  order=None,\n",
        "                  frequency=None,\n",
        "                  sampling_rate=1000., **kwargs):\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify a signal to filter.\")\n",
        "\n",
        "    # get filter\n",
        "    b, a = get_filter(ftype=ftype,\n",
        "                      order=order,\n",
        "                      frequency=frequency,\n",
        "                      sampling_rate=sampling_rate,\n",
        "                      band=band, **kwargs)\n",
        "\n",
        "    # filter\n",
        "    filtered, _ = _filter_signal(b, a, signal, check_phase=True)\n",
        "\n",
        "    # output\n",
        "    params = {\n",
        "        'ftype': ftype,\n",
        "        'order': order,\n",
        "        'frequency': frequency,\n",
        "        'band': band,\n",
        "    }\n",
        "    params.update(kwargs)\n",
        "\n",
        "    args = (filtered, sampling_rate, params)\n",
        "    names = ('signal', 'sampling_rate', 'params')\n",
        "\n",
        "    return ReturnTuple(args, names)\n",
        "def find_extrema(signal=None, mode='both'):\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    if mode not in ['max', 'min', 'both']:\n",
        "        raise ValueError(\"Unknwon mode %r.\" % mode)\n",
        "\n",
        "    aux = np.diff(np.sign(np.diff(signal)))\n",
        "\n",
        "    if mode == 'both':\n",
        "        aux = np.abs(aux)\n",
        "        extrema = np.nonzero(aux > 0)[0] + 1\n",
        "    elif mode == 'max':\n",
        "        extrema = np.nonzero(aux < 0)[0] + 1\n",
        "    elif mode == 'min':\n",
        "        extrema = np.nonzero(aux > 0)[0] + 1\n",
        "\n",
        "    values = signal[extrema]\n",
        "\n",
        "    return ReturnTuple((extrema, values), ('extrema', 'values'))\n",
        "def _get_window(kernel, size, **kwargs):\n",
        "    # mimics scipy.signal.get_window\n",
        "    if kernel in ['blackman', 'black', 'blk']:\n",
        "        winfunc = ss.blackman\n",
        "    elif kernel in ['triangle', 'triang', 'tri']:\n",
        "        winfunc = ss.triang\n",
        "    elif kernel in ['hamming', 'hamm', 'ham']:\n",
        "        winfunc = ss.hamming\n",
        "    elif kernel in ['bartlett', 'bart', 'brt']:\n",
        "        winfunc = ss.bartlett\n",
        "    elif kernel in ['hanning', 'hann', 'han']:\n",
        "        winfunc = ss.hann\n",
        "    elif kernel in ['blackmanharris', 'blackharr', 'bkh']:\n",
        "        winfunc = ss.blackmanharris\n",
        "    elif kernel in ['parzen', 'parz', 'par']:\n",
        "        winfunc = ss.parzen\n",
        "    elif kernel in ['bohman', 'bman', 'bmn']:\n",
        "        winfunc = ss.bohman\n",
        "    elif kernel in ['nuttall', 'nutl', 'nut']:\n",
        "        winfunc = ss.nuttall\n",
        "    elif kernel in ['barthann', 'brthan', 'bth']:\n",
        "        winfunc = ss.barthann\n",
        "    elif kernel in ['flattop', 'flat', 'flt']:\n",
        "        winfunc = ss.flattop\n",
        "    elif kernel in ['kaiser', 'ksr']:\n",
        "        winfunc = ss.kaiser\n",
        "    elif kernel in ['gaussian', 'gauss', 'gss']:\n",
        "        winfunc = ss.gaussian\n",
        "    elif kernel in ['general gaussian', 'general_gaussian', 'general gauss',\n",
        "                    'general_gauss', 'ggs']:\n",
        "        winfunc = ss.general_gaussian\n",
        "    elif kernel in ['boxcar', 'box', 'ones', 'rect', 'rectangular']:\n",
        "        winfunc = ss.boxcar\n",
        "    elif kernel in ['slepian', 'slep', 'optimal', 'dpss', 'dss']:\n",
        "        winfunc = ss.slepian\n",
        "    elif kernel in ['cosine', 'halfcosine']:\n",
        "        winfunc = ss.cosine\n",
        "    elif kernel in ['chebwin', 'cheb']:\n",
        "        winfunc = ss.chebwin\n",
        "    else:\n",
        "        raise ValueError(\"Unknown window type.\")\n",
        "\n",
        "    try:\n",
        "        window = winfunc(size, **kwargs)\n",
        "    except TypeError as e:\n",
        "        raise TypeError(\"Invalid window arguments: %s.\" % e)\n",
        "\n",
        "    return window\n",
        "def smoother(signal=None, kernel='boxzen', size=10, mirror=True, **kwargs):\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify a signal to smooth.\")\n",
        "\n",
        "    length = len(signal)\n",
        "\n",
        "    if isinstance(kernel, six.string_types):\n",
        "        # check length\n",
        "        if size > length:\n",
        "            size = length - 1\n",
        "\n",
        "        if size < 1:\n",
        "            size = 1\n",
        "\n",
        "        if kernel == 'boxzen':\n",
        "            # hybrid method\n",
        "            # 1st pass - boxcar kernel\n",
        "            aux, _ = smoother(signal,\n",
        "                              kernel='boxcar',\n",
        "                              size=size,\n",
        "                              mirror=mirror)\n",
        "\n",
        "            # 2nd pass - parzen kernel\n",
        "            smoothed, _ = smoother(aux,\n",
        "                                   kernel='parzen',\n",
        "                                   size=size,\n",
        "                                   mirror=mirror)\n",
        "\n",
        "            params = {'kernel': kernel, 'size': size, 'mirror': mirror}\n",
        "\n",
        "            args = (smoothed, params)\n",
        "            names = ('signal', 'params')\n",
        "\n",
        "            return ReturnTuple(args, names)\n",
        "\n",
        "        elif kernel == 'median':\n",
        "            # median filter\n",
        "            if size % 2 == 0:\n",
        "                raise ValueError(\n",
        "                    \"When the kernel is 'median', size must be odd.\")\n",
        "\n",
        "            smoothed = ss.medfilt(signal, kernel_size=size)\n",
        "\n",
        "            params = {'kernel': kernel, 'size': size, 'mirror': mirror}\n",
        "\n",
        "            args = (smoothed, params)\n",
        "            names = ('signal', 'params')\n",
        "\n",
        "            return ReturnTuple(args, names)\n",
        "\n",
        "        else:\n",
        "            win = _get_window(kernel, size, **kwargs)\n",
        "\n",
        "    elif isinstance(kernel, np.ndarray):\n",
        "        win = kernel\n",
        "        size = len(win)\n",
        "\n",
        "        # check length\n",
        "        if size > length:\n",
        "            raise ValueError(\"Kernel size is bigger than signal length.\")\n",
        "\n",
        "        if size < 1:\n",
        "            raise ValueError(\"Kernel size is smaller than 1.\")\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"Unknown kernel type.\")\n",
        "\n",
        "    # convolve\n",
        "    w = win / win.sum()\n",
        "    if mirror:\n",
        "        aux = np.concatenate(\n",
        "            (signal[0] * np.ones(size), signal, signal[-1] * np.ones(size)))\n",
        "        smoothed = np.convolve(w, aux, mode='same')\n",
        "        smoothed = smoothed[size:-size]\n",
        "    else:\n",
        "        smoothed = np.convolve(w, signal, mode='same')\n",
        "\n",
        "    # output\n",
        "    params = {'kernel': kernel, 'size': size, 'mirror': mirror}\n",
        "    params.update(kwargs)\n",
        "\n",
        "    args = (smoothed, params)\n",
        "    names = ('signal', 'params')\n",
        "\n",
        "    return ReturnTuple(args, names)\n",
        "def hamilton_segmenter(signal=None, sampling_rate=1000.):\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    sampling_rate = float(sampling_rate)\n",
        "    length = len(signal)\n",
        "    dur = length / sampling_rate\n",
        "\n",
        "    # algorithm parameters\n",
        "    v1s = int(1. * sampling_rate)\n",
        "    v100ms = int(0.1 * sampling_rate)\n",
        "    TH_elapsed = np.ceil(0.36 * sampling_rate)\n",
        "    sm_size = int(0.08 * sampling_rate)\n",
        "    init_ecg = 8  # seconds for initialization\n",
        "    if dur < init_ecg:\n",
        "        init_ecg = int(dur)\n",
        "\n",
        "    # filtering\n",
        "    filtered, _, _ = filter_signal(signal=signal,\n",
        "                                      ftype='butter',\n",
        "                                      band='lowpass',\n",
        "                                      order=4,\n",
        "                                      frequency=25.,\n",
        "                                      sampling_rate=sampling_rate)\n",
        "    filtered, _, _ = filter_signal(signal=filtered,\n",
        "                                      ftype='butter',\n",
        "                                      band='highpass',\n",
        "                                      order=4,\n",
        "                                      frequency=3.,\n",
        "                                      sampling_rate=sampling_rate)\n",
        "\n",
        "    # diff\n",
        "    dx = np.abs(np.diff(filtered, 1) * sampling_rate)\n",
        "\n",
        "    # smoothing\n",
        "    dx, _ = smoother(signal=dx, kernel='hamming', size=sm_size, mirror=True)\n",
        "\n",
        "    # buffers\n",
        "    qrspeakbuffer = np.zeros(init_ecg)\n",
        "    noisepeakbuffer = np.zeros(init_ecg)\n",
        "    peak_idx_test = np.zeros(init_ecg)\n",
        "    noise_idx = np.zeros(init_ecg)\n",
        "    rrinterval = sampling_rate * np.ones(init_ecg)\n",
        "\n",
        "    a, b = 0, v1s\n",
        "    all_peaks, _ = find_extrema(signal=dx, mode='max')\n",
        "    for i in range(init_ecg):\n",
        "        peaks, values = find_extrema(signal=dx[a:b], mode='max')\n",
        "        try:\n",
        "            ind = np.argmax(values)\n",
        "        except ValueError:\n",
        "            pass\n",
        "        else:\n",
        "            # peak amplitude\n",
        "            qrspeakbuffer[i] = values[ind]\n",
        "            # peak location\n",
        "            peak_idx_test[i] = peaks[ind] + a\n",
        "\n",
        "        a += v1s\n",
        "        b += v1s\n",
        "\n",
        "    # thresholds\n",
        "    ANP = np.median(noisepeakbuffer)\n",
        "    AQRSP = np.median(qrspeakbuffer)\n",
        "    TH = 0.475\n",
        "    DT = ANP + TH * (AQRSP - ANP)\n",
        "    DT_vec = []\n",
        "    indexqrs = 0\n",
        "    indexnoise = 0\n",
        "    indexrr = 0\n",
        "    npeaks = 0\n",
        "    offset = 0\n",
        "\n",
        "    beats = []\n",
        "\n",
        "    # detection rules\n",
        "    # 1 - ignore all peaks that precede or follow larger peaks by less than 200ms\n",
        "    lim = int(np.ceil(0.2 * sampling_rate))\n",
        "    diff_nr = int(np.ceil(0.045 * sampling_rate))\n",
        "    bpsi, bpe = offset, 0\n",
        "\n",
        "    for f in all_peaks:\n",
        "        DT_vec += [DT]\n",
        "        # 1 - Checking if f-peak is larger than any peak following or preceding it by less than 200 ms\n",
        "        peak_cond = np.array((all_peaks > f - lim) * (all_peaks < f + lim) * (all_peaks != f))\n",
        "        peaks_within = all_peaks[peak_cond]\n",
        "        if peaks_within.any() and (max(dx[peaks_within]) > dx[f]):\n",
        "            continue\n",
        "\n",
        "        # 4 - If the peak is larger than the detection threshold call it a QRS complex, otherwise call it noise\n",
        "        if dx[f] > DT:\n",
        "            # 2 - look for both positive and negative slopes in raw signal\n",
        "            if f < diff_nr:\n",
        "                diff_now = np.diff(signal[0:f + diff_nr])\n",
        "            elif f + diff_nr >= len(signal):\n",
        "                diff_now = np.diff(signal[f - diff_nr:len(dx)])\n",
        "            else:\n",
        "                diff_now = np.diff(signal[f - diff_nr:f + diff_nr])\n",
        "            diff_signer = diff_now[diff_now > 0]\n",
        "            if len(diff_signer) == 0 or len(diff_signer) == len(diff_now):\n",
        "                continue\n",
        "            # RR INTERVALS\n",
        "            if npeaks > 0:\n",
        "                # 3 - in here we check point 3 of the Hamilton paper\n",
        "                # that is, we check whether our current peak is a valid R-peak.\n",
        "                prev_rpeak = beats[npeaks - 1]\n",
        "\n",
        "                elapsed = f - prev_rpeak\n",
        "                # if the previous peak was within 360 ms interval\n",
        "                if elapsed < TH_elapsed:\n",
        "                    # check current and previous slopes\n",
        "                    if prev_rpeak < diff_nr:\n",
        "                        diff_prev = np.diff(signal[0:prev_rpeak + diff_nr])\n",
        "                    elif prev_rpeak + diff_nr >= len(signal):\n",
        "                        diff_prev = np.diff(signal[prev_rpeak - diff_nr:len(dx)])\n",
        "                    else:\n",
        "                        diff_prev = np.diff(signal[prev_rpeak - diff_nr:prev_rpeak + diff_nr])\n",
        "\n",
        "                    slope_now = max(diff_now)\n",
        "                    slope_prev = max(diff_prev)\n",
        "\n",
        "                    if (slope_now < 0.5 * slope_prev):\n",
        "                        # if current slope is smaller than half the previous one, then it is a T-wave\n",
        "                        continue\n",
        "                if dx[f] < 3. * np.median(qrspeakbuffer):  # avoid retarded noise peaks\n",
        "                    beats += [int(f) + bpsi]\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if bpe == 0:\n",
        "                    rrinterval[indexrr] = beats[npeaks] - beats[npeaks - 1]\n",
        "                    indexrr += 1\n",
        "                    if indexrr == init_ecg:\n",
        "                        indexrr = 0\n",
        "                else:\n",
        "                    if beats[npeaks] > beats[bpe - 1] + v100ms:\n",
        "                        rrinterval[indexrr] = beats[npeaks] - beats[npeaks - 1]\n",
        "                        indexrr += 1\n",
        "                        if indexrr == init_ecg:\n",
        "                            indexrr = 0\n",
        "\n",
        "            elif dx[f] < 3. * np.median(qrspeakbuffer):\n",
        "                beats += [int(f) + bpsi]\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            npeaks += 1\n",
        "            qrspeakbuffer[indexqrs] = dx[f]\n",
        "            peak_idx_test[indexqrs] = f\n",
        "            indexqrs += 1\n",
        "            if indexqrs == init_ecg:\n",
        "                indexqrs = 0\n",
        "        if dx[f] <= DT:\n",
        "            # 4 - not valid\n",
        "            # 5 - If no QRS has been detected within 1.5 R-to-R intervals,\n",
        "            # there was a peak that was larger than half the detection threshold,\n",
        "            # and the peak followed the preceding detection by at least 360 ms,\n",
        "            # classify that peak as a QRS complex\n",
        "            tf = f + bpsi\n",
        "            # RR interval median\n",
        "            RRM = np.median(rrinterval)  # initial values are good?\n",
        "\n",
        "            if len(beats) >= 2:\n",
        "                elapsed = tf - beats[npeaks - 1]\n",
        "\n",
        "                if elapsed >= 1.5 * RRM and elapsed > TH_elapsed:\n",
        "                    if dx[f] > 0.5 * DT:\n",
        "                        beats += [int(f) + offset]\n",
        "                        # RR INTERVALS\n",
        "                        if npeaks > 0:\n",
        "                            rrinterval[indexrr] = beats[npeaks] - beats[npeaks - 1]\n",
        "                            indexrr += 1\n",
        "                            if indexrr == init_ecg:\n",
        "                                indexrr = 0\n",
        "                        npeaks += 1\n",
        "                        qrspeakbuffer[indexqrs] = dx[f]\n",
        "                        peak_idx_test[indexqrs] = f\n",
        "                        indexqrs += 1\n",
        "                        if indexqrs == init_ecg:\n",
        "                            indexqrs = 0\n",
        "                else:\n",
        "                    noisepeakbuffer[indexnoise] = dx[f]\n",
        "                    noise_idx[indexnoise] = f\n",
        "                    indexnoise += 1\n",
        "                    if indexnoise == init_ecg:\n",
        "                        indexnoise = 0\n",
        "            else:\n",
        "                noisepeakbuffer[indexnoise] = dx[f]\n",
        "                noise_idx[indexnoise] = f\n",
        "                indexnoise += 1\n",
        "                if indexnoise == init_ecg:\n",
        "                    indexnoise = 0\n",
        "\n",
        "        # Update Detection Threshold\n",
        "        ANP = np.median(noisepeakbuffer)\n",
        "        AQRSP = np.median(qrspeakbuffer)\n",
        "        DT = ANP + 0.475 * (AQRSP - ANP)\n",
        "\n",
        "    beats = np.array(beats)\n",
        "\n",
        "    lim = lim\n",
        "    r_beats = []\n",
        "    thres_ch = 0.85\n",
        "    adjacency = 0.05 * sampling_rate\n",
        "    for i in beats:\n",
        "        error = [False, False]\n",
        "        if i - lim < 0:\n",
        "            window = signal[0:i + lim]\n",
        "            add = 0\n",
        "        elif i + lim >= length:\n",
        "            window = signal[i - lim:length]\n",
        "            add = i - lim\n",
        "        else:\n",
        "            window = signal[i - lim:i + lim]\n",
        "            add = i - lim\n",
        "        # meanval = np.mean(window)\n",
        "        w_peaks, _ = find_extrema(signal=window, mode='max')\n",
        "        w_negpeaks, _ = find_extrema(signal=window, mode='min')\n",
        "        zerdiffs = np.where(np.diff(window) == 0)[0]\n",
        "        w_peaks = np.concatenate((w_peaks, zerdiffs))\n",
        "        w_negpeaks = np.concatenate((w_negpeaks, zerdiffs))\n",
        "\n",
        "        pospeaks = sorted(zip(window[w_peaks], w_peaks), reverse=True)\n",
        "        negpeaks = sorted(zip(window[w_negpeaks], w_negpeaks))\n",
        "\n",
        "        try:\n",
        "            twopeaks = [pospeaks[0]]\n",
        "        except IndexError:\n",
        "            pass\n",
        "        try:\n",
        "            twonegpeaks = [negpeaks[0]]\n",
        "        except IndexError:\n",
        "            pass\n",
        "\n",
        "        # getting positive peaks\n",
        "        for i in range(len(pospeaks) - 1):\n",
        "            if abs(pospeaks[0][1] - pospeaks[i + 1][1]) > adjacency:\n",
        "                twopeaks.append(pospeaks[i + 1])\n",
        "                break\n",
        "        try:\n",
        "            posdiv = abs(twopeaks[0][0] - twopeaks[1][0])\n",
        "        except IndexError:\n",
        "            error[0] = True\n",
        "\n",
        "        # getting negative peaks\n",
        "        for i in range(len(negpeaks) - 1):\n",
        "            if abs(negpeaks[0][1] - negpeaks[i + 1][1]) > adjacency:\n",
        "                twonegpeaks.append(negpeaks[i + 1])\n",
        "                break\n",
        "        try:\n",
        "            negdiv = abs(twonegpeaks[0][0] - twonegpeaks[1][0])\n",
        "        except IndexError:\n",
        "            error[1] = True\n",
        "\n",
        "        # choosing type of R-peak\n",
        "        if not sum(error):\n",
        "            if posdiv > thres_ch * negdiv:\n",
        "                # pos noerr\n",
        "                r_beats.append(twopeaks[0][1] + add)\n",
        "            else:\n",
        "                # neg noerr\n",
        "                r_beats.append(twonegpeaks[0][1] + add)\n",
        "        elif sum(error) == 2:\n",
        "            if abs(twopeaks[0][1]) > abs(twonegpeaks[0][1]):\n",
        "                # pos allerr\n",
        "                r_beats.append(twopeaks[0][1] + add)\n",
        "            else:\n",
        "                # neg allerr\n",
        "                r_beats.append(twonegpeaks[0][1] + add)\n",
        "        elif error[0]:\n",
        "            # pos poserr\n",
        "            r_beats.append(twopeaks[0][1] + add)\n",
        "        else:\n",
        "            # neg negerr\n",
        "            r_beats.append(twonegpeaks[0][1] + add)\n",
        "\n",
        "    rpeaks = sorted(list(set(r_beats)))\n",
        "    rpeaks = np.array(rpeaks, dtype='int')\n",
        "\n",
        "    return ReturnTuple((rpeaks,), ('rpeaks',))\n",
        "def _extract_heartbeats(signal=None, rpeaks=None, before=200, after=400):\n",
        "\n",
        "    R = np.sort(rpeaks)\n",
        "    length = len(signal)\n",
        "    templates = []\n",
        "    newR = []\n",
        "\n",
        "    for r in R:\n",
        "        a = r - before\n",
        "        if a < 0:\n",
        "            continue\n",
        "        b = r + after\n",
        "        if b > length:\n",
        "            break\n",
        "        templates.append(signal[a:b])\n",
        "        newR.append(r)\n",
        "\n",
        "    templates = np.array(templates)\n",
        "    newR = np.array(newR, dtype='int')\n",
        "\n",
        "    return templates, newR\n",
        "def extract_heartbeats(signal=None, rpeaks=None, sampling_rate=1000.,\n",
        "                       before=0.2, after=0.4):\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    if rpeaks is None:\n",
        "        raise TypeError(\"Please specify the input R-peak locations.\")\n",
        "\n",
        "    if before < 0:\n",
        "        raise ValueError(\"Please specify a non-negative 'before' value.\")\n",
        "    if after < 0:\n",
        "        raise ValueError(\"Please specify a non-negative 'after' value.\")\n",
        "\n",
        "    # convert delimiters to samples\n",
        "    before = int(before * sampling_rate)\n",
        "    after = int(after * sampling_rate)\n",
        "\n",
        "    # get heartbeats\n",
        "    templates, newR = _extract_heartbeats(signal=signal,\n",
        "                                          rpeaks=rpeaks,\n",
        "                                          before=before,\n",
        "                                          after=after)\n",
        "\n",
        "    return ReturnTuple((templates, newR), ('templates', 'rpeaks'))\n",
        "def get_heart_rate(beats=None, sampling_rate=1000., smooth=False, size=3):\n",
        "    # check inputs\n",
        "    if beats is None:\n",
        "        raise TypeError(\"Please specify the input beat indices.\")\n",
        "\n",
        "    if len(beats) < 2:\n",
        "        # raise ValueError(\"Not enough beats to compute heart rate.\")\n",
        "        return ReturnTuple((np.array([], dtype=np.int32), np.array([], dtype=np.int32)), ('index', 'heart_rate'))\n",
        "\n",
        "    # compute heart rate\n",
        "    ts = beats[1:]\n",
        "    hr = sampling_rate * (60. / np.diff(beats))\n",
        "\n",
        "    # physiological limits\n",
        "    indx = np.nonzero(np.logical_and(hr >= 40, hr <= 200))\n",
        "    ts = ts[indx]\n",
        "    hr = hr[indx]\n",
        "\n",
        "    # smooth with moving average\n",
        "    if smooth and (len(hr) > 1):\n",
        "        hr, _ = smoother(signal=hr, kernel='boxcar', size=size, mirror=True)\n",
        "\n",
        "    return ReturnTuple((ts, hr), ('index', 'heart_rate'))\n",
        "def ecg(signal=None, sampling_rate=1000., show=True):\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    # ensure numpy\n",
        "    signal = np.array(signal)\n",
        "\n",
        "    sampling_rate = float(sampling_rate)\n",
        "\n",
        "    # filter signal\n",
        "    order = int(0.3 * sampling_rate)\n",
        "    filtered, _, _ = filter_signal(signal=signal,\n",
        "                                      ftype='FIR',\n",
        "                                      band='bandpass',\n",
        "                                      order=order,\n",
        "                                      frequency=[3, 45],\n",
        "                                      sampling_rate=sampling_rate)\n",
        "\n",
        "    # segment\n",
        "    rpeaks, = hamilton_segmenter(signal=filtered, sampling_rate=sampling_rate)\n",
        "\n",
        "    # extract templates\n",
        "    templates, rpeaks = extract_heartbeats(signal=filtered,\n",
        "                                           rpeaks=rpeaks,\n",
        "                                           sampling_rate=sampling_rate,\n",
        "                                           before=0.2,\n",
        "                                           after=0.4)\n",
        "\n",
        "    # compute heart rate\n",
        "    hr_idx, hr = get_heart_rate(beats=rpeaks,\n",
        "                                   sampling_rate=sampling_rate,\n",
        "                                   smooth=True,\n",
        "                                   size=3)\n",
        "\n",
        "    if len(hr_idx) > 0:\n",
        "        # get time vectors\n",
        "        length = len(signal)\n",
        "        T = (length - 1) / sampling_rate\n",
        "        ts = np.linspace(0, T, length, endpoint=False)\n",
        "        ts_hr = ts[hr_idx]\n",
        "        ts_tmpl = np.linspace(-0.2, 0.4, templates.shape[1], endpoint=False)\n",
        "    else:\n",
        "        ts = np.array([])\n",
        "        ts_hr = np.array([])\n",
        "        ts_tmpl = np.array([])\n",
        "\n",
        "    # plot\n",
        "    # if show:\n",
        "    #     plot(ts=ts,\n",
        "    #          signal=signal,\n",
        "    #          filtered=filtered,\n",
        "    #          rpeaks=rpeaks,\n",
        "    #          ts_tmpl=ts_tmpl,\n",
        "    #          templates=templates,\n",
        "    #          ts_hr=ts_hr,\n",
        "    #          hr=hr)\n",
        "\n",
        "    # output\n",
        "    args = (ts, filtered, rpeaks, ts_tmpl, templates, ts_hr, hr)\n",
        "    names = ('ts', 'filtered', 'rpeaks', 'templates_ts', 'templates',\n",
        "             'heart_rate_ts', 'heart_rate')\n",
        "\n",
        "    return ReturnTuple(args, names)\n",
        "def get_features_dict(x):\n",
        "    [ts, fts, rpeaks, tts, thb, hrts, hr] = ecg(signal=x, sampling_rate=FREQUENCY, show=False)\n",
        "\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "    ts (array) – Signal time axis reference (seconds).\n",
        "    filtered (array) – Filtered ECG signal.\n",
        "    rpeaks (array) – R-peak location indices.\n",
        "    templates_ts (array) – Templates time axis reference (seconds).\n",
        "    templates (array) – Extracted heartbeat templates.\n",
        "    heart_rate_ts (array) – Heart rate time axis reference (seconds).\n",
        "    heart_rate (array) – Instantaneous heart rate (bpm).\n",
        "    \"\"\"\n",
        "\n",
        "    fx = dict()\n",
        "    fx.update(heart_rate_features(hr))\n",
        "    fx.update(frequency_powers(x, n_power_features=60))\n",
        "    fx.update(add_suffix(frequency_powers(fts), \"fil\"))\n",
        "    fx.update(frequency_powers_summary(fts))\n",
        "    fx.update(heart_beats_features2(thb))\n",
        "    fx.update(fft_features(median_heartbeat(thb)))\n",
        "    # fx.update(heart_beats_features3(thb))\n",
        "    fx.update(r_features(fts, rpeaks))\n",
        "\n",
        "    fx['PRbyST'] = fx['PR_interval'] * fx['ST_interval']\n",
        "    fx['P_form'] = fx['P_kurt'] * fx['P_skew']\n",
        "    fx['T_form'] = fx['T_kurt'] * fx['T_skew']\n",
        "\n",
        "    for key, value in fx.items():\n",
        "        if np.math.isnan(value):\n",
        "            value = 0\n",
        "        fx[key] = value\n",
        "\n",
        "    return fx\n",
        "\n",
        "\n",
        "def get_feature_names(x):\n",
        "    features = get_features_dict(x)\n",
        "    return sorted(list(features.keys()))\n",
        "\n",
        "\n",
        "def features_for_row(x):\n",
        "    features = get_features_dict(x)\n",
        "    return np.array([features[key] for key in sorted(list(features.keys()))], dtype=np.float32)"
      ],
      "metadata": {
        "id": "V4lNjfM0xyf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This Part Contains Below Steps:\n",
        "##### 1. Loading Data From Our Developed Device\n",
        "##### 2. Upsampling Our ECG Data from 130Hz to 300Hz\n",
        "##### 3. Passing Through a Low-Pass Filter to Compensate the Potential Alising Resulted From Upsampling\n",
        "##### 4. Preprocessing\n",
        "##### 5. Feature Extraction\n",
        "##### 6. Loading Scalar Weights for Normalizing features\n",
        "##### 7. Loading Saved MLP Model and Inferencing"
      ],
      "metadata": {
        "id": "V9aSIPENUgRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.signal import resample, butter, filtfilt\n",
        "\n",
        "def classification(data_dir):\n",
        "    ######################################################################################\n",
        "    # Loading our own normal ecg data\n",
        "    ecg_signal = scipy.io.loadmat(data_dir)\n",
        "    ecg_signal = ecg_signal['ecg_data'][0]\n",
        "    ######################################################################################\n",
        "    # upsample our ecg data from sampling rate of 130Hz to sampling rate of 300Hz\n",
        "    original_sampling_rate = 130\n",
        "    target_sampling_rate = 300\n",
        "    cutoff_frequency = 45\n",
        "\n",
        "    # Create a low-pass filter with a cutoff frequency of \"45\" Hz\n",
        "    b, a = butter(8, cutoff_frequency / (original_sampling_rate / 2), btype='lowpass')\n",
        "    filtered_signal = filtfilt(b, a, ecg_signal)\n",
        "\n",
        "    # Determine the length of the upsampled signal\n",
        "    upsampled_length = int((len(filtered_signal) / original_sampling_rate) * target_sampling_rate)\n",
        "\n",
        "    # Upsample the filtered signal\n",
        "    upsampled_signal = resample(filtered_signal, upsampled_length)\n",
        "\n",
        "    #####################################################################################\n",
        "    #preprocessing\n",
        "    subX = normalize_batch([upsampled_signal])\n",
        "    #feature extraction\n",
        "    # print(\"Features extraction started\")\n",
        "    features = apply_async(subX, features_for_row)\n",
        "    # print(\"Features extraction finished\", len(features[0]))\n",
        "\n",
        "    #####################################################################################\n",
        "    # load the scaler object from file to normalize our features\n",
        "    with open('/content/drive/MyDrive/scaler.pkl', 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    X_new_scaled = scaler.transform(features)\n",
        "\n",
        "    #####################################################################################\n",
        "    # load the saved model from google drive using pickle.load()\n",
        "    with open('/content/drive/MyDrive/Trained_clf_ECG/mlp1_model.pkl', 'rb') as f:\n",
        "        clf_mlp = pickle.load(f)\n",
        "\n",
        "    #####################################################################################\n",
        "    # use the loaded model for inference\n",
        "    y_pred = clf_mlp.predict(X_new_scaled)\n",
        "    if y_pred == 1:\n",
        "      print('Predicted label is Normal')\n",
        "    elif y_pred == 0:\n",
        "      print('Predicted label is Atrial Fibrilation')\n",
        "    else:\n",
        "      print('Something is wrong!')\n"
      ],
      "metadata": {
        "id": "9TRyvEPk2BYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/testECG_received from hossein/ecg2.mat\"\n",
        "classification(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI6Vr3pdXVxU",
        "outputId": "6e6ebed6-fa6b-4616-b5cd-6c320ed447e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-8cb39828679c>:63: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
            "  return stats.mode(a, axis=None)[0][0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label is Normal\n"
          ]
        }
      ]
    }
  ]
}